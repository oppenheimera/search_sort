\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.8in]{geometry}
\usepackage{layout}
\usepackage{dirtytalk}
\begin{document}
\section{Sort}
\subsection{Insertion Sort}
Insertion sort works by scanning from right to left and when an item is out of place, \say{bubbling} that item to the left until it is in its rightful place.
\newline \newline \noindent
Basically what happens is an item (the leftmost of the unsorted portion) is selected, and then scanned through the already sorted portion of the list until its rightful place is found.
\begin{itemize}
\item stable? Yes.
\item time complexity:
Worst case is $\Theta n^2$.
\item space complexity: 
$\Theta 1$ as everything is done in place.
\item best-case input:
A nearly-sorted array, as almost all needed work is done. 
\item worst-case input: An array in reverse order. $\sum_{k=1}^{n} = 1 + 2 + 3 + ... + n$, because the item we are looking at \textit{belongs} at the new beginning of the list, but we have to look at \textit{everything} on the way over.
\end{itemize}
\subsection{Selection Sort}
Selection sort scans through the entire \emph{unsorted} part of the list looking for the next smallest item. Therefore the running-time of this algorithm is given by the series $\sum_{k=1}^n k$ where $k$ is the number of compares.
\begin{itemize}
\item Not stable
\item Upper-bounded by $O(n^2)$, actually, always runs in $\Theta(n^2)$
\item The only benefit of selection sort over other sorts: it minimizes the number of array swaps
\end{itemize}
\subsection{HeapSort}
Heapifies data (classically bottom-up), then builds the sorted list from the last item to the first.
\newline \newline \noindent
Runs in absolute worst case $O\big(n\cdot lg(n)\big)$, so good choice if time constraints are tight and Quicksort with shuffle can't be used.
\begin{itemize}
\item Not stable
\item $\Theta\big(n\cdot lg(n)\big)$
\end{itemize}
\subsection{MergeSort}
The fkn shit.
\begin{itemize}
\item Stable
\item $\Theta\big(n\cdot lg(n)\big)$
\item $\Theta (n)$ extra space for arrays
\item$\Theta\big(lg(n)\big)$ extra space for your mergeable datastructure of choice
\end{itemize}
\subsection{QuickSort}
Picks an item as a pivot (this was classically the leftmostmost item, but this causes worst case performance on already sorted input). Partitions $x$ around pivot $n$ like dis: 
\begin{tabular}{| c | c | c |}
\hline
$x < n$ & $x = n$ & $n<x$ \\
\hline
\end{tabular}
\newline \newline \noindent
Naive implementations run in $n^2$ on already sorted arrays (which is apparently common in real world situations), so caution must be exercised with this sort.
\newline \newline \noindent
There are many situations where we might use Quicksort: When we want faster overall runtime, better memory usage, when we don't care about stability, or when we have many duplicates.
\begin{itemize}
\item Not stable\footnote{sometimes it is, but sketchy things must be done for this to be the case.}
\item Upper-bounded by $O(n^2)$ on already sorted (and not shuffled arrays, but is pretty dependably $\Theta\big(n\cdot log(n)\big)$ with carefullness
\end{itemize}
\subsection{MSD \& LSD}
Items are sorted by \emph{Least Significant Digit} or \emph{Most Significant Digit}. Arrays are partitioned into subarrays based on digit place being looked at (i.e. tens, hundreds, thousands) and placed in order based on that place. Stability ensures that they are in place.
\newpage
\section{Search}
Adjacency list: list of nodes that can be \emph{immediately} reached from the current node. A.K.A. connections or outgoing edges.

Adjacency matrix: table or matrix of \emph{nodes} $\cdot$ \emph{nodes} where each intersection is either \emph{true} or \emph{false}

\begin{center}
\begin{tabular}{c | c c c}
 & a & b & c \\ \hline
 a & 0 & 1 & 1 \\
 b & 1 & 0 & 0 \\
 c & 1 & 0 & 0 \\
\end{tabular}
\end{center}

\subsection{Djikstra's}
time complexity: $O(|E|log|V|)$ according to Hug, and $O(|E| + |V|log|V|)$ according to Wikipedia. Hug's justification is as follow::
\begin{itemize}
\item we do $V$ insertions, which each cost $O(log|V|)$ time.
\item we do $V$ min-deletions, costing $O(log|V|)$ time each.
\item we do $E$ \say{priority decreases}, costing $O(log|V|)$ time each.
\item so our total runtime is $O(V\cdot log|V| + V\cdot log|V| + E\cdot log|V|) \Rightarrow O(2V+E\cdot log|V|)$
\item which becomes $O(E\cdot log|V|)$
\end{itemize}

Algorithm for finding shortest path in a graph. \textit{Greedy} algorithm in that it enqueues the lowest cost path th
\subsection{A*}
Djisktra's but with heuristics added to edge-weights.
\subsection{Kruskal's}
time complexity: $V$ is at most $E^2$, so it follows that 
$O(logV^2) \Rightarrow O(2logV) \Rightarrow O(logV)$.

Algorithm for finding the MST of a graph. 

Pseudocode: 
\begin{verbatim}
insert all edges into MinPQ
while (not all nodes are connected):
    dequeue edge
    if (edge doesn't create a cycle):
    	    add edge
\end{verbatim}
One issue with this algorithm is that it runs the risk of adding edges that are disconnected to the larger whole of the graph if not careful about enqueuing edges. So for Kruskal's to be the ideal algorithm we must be sure that the graph is connected.
\subsection{Primm's}
time complexity: 
\begin{itemize}
\item with binary heap and adjacency list: $O (|E|\ log\ |V|)$
\item with adjacency matrix $O (|V|^2)$
\end{itemize}


Another algorithm for finding the MST of a graph. Algorithm of choice if not sure that graph is connected. As Primm's is run from an arbitrary vertex, it will only find vertices that are connected to starting vertex. 

Pseudocode:
\begin{verbatim}
start at a vertex
while (MST isn't found):
    visit closest unvisited vertex to any visited vertex
    add edge to MST
\end{verbatim}
So what happens is essentially that we pick a place to start, then visit the node closest to the start if we haven't already visited it. We will then continue to do this until we've found the MST for the graph.
\section{Addenda}
\subsection{Asymptotics}

\begin{equation}
Taking:
\frac{r(\alpha)}{r(\beta)} = \bigg(\frac{\alpha}{\beta}\bigg)^b as\ \gamma = \delta
\end{equation}
\begin{equation}
we\ can\ siplify\ to:\ log_\gamma \delta = b \textrm{ and solve for $b$}
\end{equation}


\end{document}